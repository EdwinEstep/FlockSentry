{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylKkR09T2KCW"
      },
      "source": [
        "# Download yolo "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHx-iUp91ir2",
        "outputId": "9e4c6b9e-691b-4f6d-8910-13f73d2d3aaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setup complete. Using torch 1.10.0+cu102 (NVIDIA GeForce GTX 1650 Ti with Max-Q Design)\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt  # install dependencies\n",
        "\n",
        "import torch\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "\n",
        "clear_output()\n",
        "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwuytkP02Ld1"
      },
      "source": [
        "# Download our training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyu9gVGJ3I6S",
        "outputId": "a69281d0-2be4-4595-d0ef-7e226e5c4406"
      },
      "outputs": [],
      "source": [
        "input(\"Download training data from drive and unpack into `all_image_data`.  Press ENTER when done.\")\n",
        "training_data_path = '../all_image_data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3oJUab0GvM0",
        "outputId": "ffb02032-5395-43b3-e107-5db567d6e082"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['joshdw_20220217_19-16-45.txt', 'joshdw_20220217_19-18-06.txt', 'joshdw_20220217_19-19-27.txt', 'joshdw_20220217_19-15-08.png', 'joshdw_20220217_19-19-06.txt', 'joshdw_20220217_19-12-29.txt', 'joshdw_20220217_19-15-42.png', 'joshdw_20220217_19-19-06.png', 'joshdw_20220217_19-13-35.png', 'joshdw_20220217_19-14-30.png', 'joshdw_20220217_19-16-22.png', 'joshdw_20220217_19-12-55.png', 'joshdw_20220217_19-19-27.png', 'joshdw_20220217_19-12-29.png', 'joshdw_20220217_19-15-08.txt', 'joshdw_20220217_19-17-23.png', 'joshdw_20220217_19-17-23.txt', 'joshdw_20220217_19-08-31.txt', 'joshdw_20220217_19-15-42.txt', 'joshdw_20220217_19-14-30.txt', 'joshdw_20220217_19-16-45.png', 'joshdw_20220217_19-12-55.txt', 'joshdw_20220217_19-18-06.png', 'joshdw_20220217_19-13-35.txt', 'joshdw_20220217_19-16-22.txt']\n"
          ]
        }
      ],
      "source": [
        "print(os.listdir(training_data_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8PVadMn24B0"
      },
      "source": [
        "# Download coco data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "f64029bb3e57491690aa9ec75aa74fcf",
            "5cc83896336e4342b157a6a1b6d43dfd",
            "cf756ea9ba02491b95c73853cc9325c0",
            "e3ace0d513a24c2ca10ff09920646c7d",
            "858681e2a3834c189245b1c38e9ae7c5",
            "68592f9fe10440129343f09d49e4667b",
            "eebf44ba085c4a0f9f04e6b2c1d1aa9f",
            "6e4fe94d643c4695be768c15ee1182e0",
            "ff4aa7f3af734260a06b946a6205a6ba",
            "7d4bcc8e79c94838a26f5b0c7ec87a0d",
            "e1bf1d7f96d44ab0808650bd7d55c47d"
          ]
        },
        "id": "XVxoiLdA3JXq",
        "outputId": "21063a4c-0ccd-4378-8d5a-5ac7a8f82da9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 780M/780M [01:44<00:00, 7.82MB/s] \n"
          ]
        }
      ],
      "source": [
        "# Download COCO val\n",
        "torch.hub.download_url_to_file('https://ultralytics.com/assets/coco2017val.zip', 'tmp.zip')\n",
        "!unzip -oq tmp.zip -d ../datasets && rm tmp.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "F1Gf2SoR_LiP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "label_directory = '../datasets/coco/labels/val2017'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dvFkcY87nQte"
      },
      "outputs": [],
      "source": [
        "#person, bird, cat, dog, horse, sheep, cow, bear\n",
        "keep = [\"0\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"21\"]\n",
        "replace_dict = {\"0\": \"4\", \"14\": \"3\", \"15\": \"5\", \"16\": \"6\", \"17\": \"7\", \"18\": \"8\", \"19\": \"9\", \"21\": \"10\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "i_dgP_-YF82T"
      },
      "outputs": [],
      "source": [
        "for filename in os.listdir(label_directory):\n",
        "  #lines we want to keep\n",
        "  useful_lines = []\n",
        "  rIdx = 0\n",
        "  wIdx = 0 \n",
        "  #read all the lines in the file\n",
        "  with open(label_directory + '/' + filename, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "      items = line.split(\" \") #items[0] = class\n",
        "      if(items[0] in keep):\n",
        "        useful_lines.append(rIdx) #note that line (all other lines will be removed)\n",
        "      rIdx = rIdx + 1\n",
        "  #copy only the lines we want to keep back to the file \n",
        "  with open(label_directory + '/'  + filename, \"w\") as f:\n",
        "    for line in lines:\n",
        "        if wIdx in useful_lines:\n",
        "            items = line.split(\" \") #items[0] = class\n",
        "            items[0] = replace_dict[items[0]]\n",
        "            new_line = \" \".join(items)\n",
        "            f.write(new_line)\n",
        "        wIdx = wIdx + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8m2Tqdygbg__"
      },
      "outputs": [],
      "source": [
        "#remove empty text files and the images that are associated with them\n",
        "for filename in os.listdir(label_directory):\n",
        "    label_path = label_directory + \"/\" + filename\n",
        "    if(os.path.getsize(label_path) == 0):\n",
        "      os.remove(label_path)\n",
        "      image_path = \"../datasets/coco/images/val2017/\" + filename.replace(\".txt\", \".jpg\")\n",
        "      os.remove(image_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPDmitnbVmgm"
      },
      "source": [
        "# Remove duplicate images from our training data\n",
        "\n",
        "It is possible that two team members downloaded and annotated the same image from the internet.  This should clean that up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1plJgFaZVmC4",
        "outputId": "63f8fe50-c23f-48ba-afb0-cf56c3c7ea67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in /home/joshdw/.local/lib/python3.9/site-packages (4.5.3.56)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /home/joshdw/.local/lib/python3.9/site-packages (from opencv-python) (1.22.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install opencv-python\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "CHICKEN_DATA_PATH = \"../all_image_data/\"\n",
        "duplicate_images = []\n",
        "\n",
        "# Gather filenames of all images\n",
        "img_fnames = [fname for fname in os.listdir(CHICKEN_DATA_PATH)\n",
        "    if fname.endswith(\".jpg\") or fname.endswith(\".png\") or fname.endswith(\".jpeg\")]\n",
        "\n",
        "chicken_images = []\n",
        "i = 0\n",
        "for img_fname in img_fnames:  # Open, normalize, and append all images\n",
        "    img = cv2.resize(cv2.imread(CHICKEN_DATA_PATH + img_fname), (640, 480))\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    chicken_images.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "391-kbu4dxKJ"
      },
      "outputs": [],
      "source": [
        "# Find images that look like the exact same image and nominate them for deletion\n",
        "for i, img in enumerate(chicken_images):\n",
        "    for j, other_img in enumerate(chicken_images[i+1:]):\n",
        "        # Test similarity with root means squared\n",
        "        diff = img - other_img\n",
        "        distance = np.sqrt(np.sum(np.square(diff)))\n",
        "        if distance < 100:\n",
        "            # Display both images\n",
        "            plt.imshow(img)\n",
        "            plt.show()\n",
        "            plt.imshow(other_img)\n",
        "            plt.show()\n",
        "            other_fname = img_fnames[i+1+j]\n",
        "            # Ask if the second image should be deleted ...\n",
        "            print(f\"Duplicate images: {img_fnames[i]} and {other_fname}\")\n",
        "            should_delete = input(f\"Delete {other_fname}? [Y/n]\")\n",
        "            # ... and delete the second image if yes\n",
        "            if should_delete in [\"y\", \"Y\", \"\"]:\n",
        "                print(f\"Deleting {other_fname} and matching label file...\")\n",
        "                os.remove(CHICKEN_DATA_PATH + \"\".join(other_fname.split(\".\")[:-1]) + \".txt\")\n",
        "                os.remove(CHICKEN_DATA_PATH + other_fname)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-14sGVH2-L6"
      },
      "source": [
        "# Allocate a certain amount for each category\n",
        "#### to not flood the chicken images "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8s0xACr3J1h",
        "outputId": "644b8078-78dd-495c-ecbc-d46f3a550bdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Custom images: 2 val, 11 train\n",
            "Coco images: 0 val, 0 train\n",
            "No matching .jpg or .png file for ../all_image_data/joshdw_20220217_19-08-31.txt.  Continuing...\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Moves a random sample of one tenth of images and labels to the validation data\n",
        "folder, and the rest to the training data folder.\n",
        "\"\"\"\n",
        "\n",
        "import shutil\n",
        "from glob import glob\n",
        "import os\n",
        "import random\n",
        "\n",
        "# Tuneable parameters for data amounts\n",
        "VALIDATION_PROPORTION = 0.2\n",
        "COCO_PROPORTION = 0.0\n",
        "\n",
        "YOLO_DATA_PATH = \"./data/\"\n",
        "COCO_DATA_PATH = \"../datasets/coco/\"\n",
        "\n",
        "# Find all of the custom data label files\n",
        "custom_label_fnames = glob(CHICKEN_DATA_PATH + \"*.txt\")\n",
        "n = int(len(custom_label_fnames) * VALIDATION_PROPORTION)\n",
        "custom_validation_labels = random.sample(custom_label_fnames, n)\n",
        "print(f\"Custom images: {n} val, {len(custom_label_fnames) - n} train\")\n",
        "\n",
        "# Find all of the coco data label files\n",
        "coco_label_fnames = glob(COCO_DATA_PATH + \"labels/val2017/*.txt\")\n",
        "n = int(len(custom_label_fnames)*COCO_PROPORTION/(1-COCO_PROPORTION))\n",
        "coco_label_fnames = random.sample(coco_label_fnames, n)\n",
        "n = int(len(coco_label_fnames) * VALIDATION_PROPORTION)\n",
        "coco_validation_labels = random.sample(coco_label_fnames, n)\n",
        "print(f\"Coco images: {n} val, {len(coco_label_fnames) - n} train\")\n",
        "\n",
        "# Clear the yolov5/data/images and yolov5/data/labels folders\n",
        "try:\n",
        "    shutil.rmtree(YOLO_DATA_PATH + \"images/\")\n",
        "    shutil.rmtree(YOLO_DATA_PATH + \"labels/\")\n",
        "except FileNotFoundError as e:  # We have already deleted the folders\n",
        "    print(f\"Not deleting nonexistent images/ and labels/ folders in {YOLO_DATA_PATH}.\")\n",
        "os.mkdir(YOLO_DATA_PATH + \"images/\")\n",
        "os.mkdir(YOLO_DATA_PATH + \"labels/\")\n",
        "os.mkdir(YOLO_DATA_PATH + \"images/val\")\n",
        "os.mkdir(YOLO_DATA_PATH + \"images/train\")\n",
        "os.mkdir(YOLO_DATA_PATH + \"labels/val\")\n",
        "os.mkdir(YOLO_DATA_PATH + \"labels/train\")\n",
        "\n",
        "def get_matching_img_fname(label_fname):\n",
        "    \"\"\"Takes a .txt label filename and returns its matching image filename\"\"\"\n",
        "    label_fname = label_fname.replace('coco/labels/val2017/', 'coco/images/val2017/')\n",
        "    img_fname = \"\"\n",
        "    label_fname_root = label_fname.replace(\".txt\", \"\")\n",
        "    if os.path.exists(f\"{label_fname_root}.jpg\"):\n",
        "        img_fname = f\"{label_fname_root}.jpg\"\n",
        "    elif os.path.exists(f\"{label_fname_root}.jpeg\"):\n",
        "        img_fname = f\"{label_fname_root}.jpeg\"\n",
        "    elif os.path.exists(f\"{label_fname_root}.png\"):\n",
        "        img_fname = f\"{label_fname_root}.png\"\n",
        "    else:\n",
        "        print(f\"No matching .jpg or .png file for {label_fname}.  Continuing...\")\n",
        "        return None\n",
        "    return img_fname\n",
        "\n",
        "# TODO: resume commenting here\n",
        "for label_fname in custom_label_fnames:\n",
        "    img_fname = get_matching_img_fname(label_fname)\n",
        "    if not img_fname:\n",
        "        continue\n",
        "\n",
        "    if label_fname in custom_validation_labels:\n",
        "        shutil.copy2(img_fname, YOLO_DATA_PATH + \"images/val/\")\n",
        "        shutil.copy2(label_fname, YOLO_DATA_PATH + \"labels/val/\")\n",
        "    else:\n",
        "        shutil.copy2(img_fname, YOLO_DATA_PATH + \"images/train/\")\n",
        "        shutil.copy2(label_fname, YOLO_DATA_PATH + \"labels/train/\")\n",
        "\n",
        "for label_fname in coco_label_fnames:\n",
        "    img_fname = get_matching_img_fname(label_fname)\n",
        "    if not img_fname:\n",
        "        continue\n",
        "\n",
        "    if label_fname in coco_validation_labels:\n",
        "        shutil.copy2(img_fname, YOLO_DATA_PATH + \"images/val/\")\n",
        "        shutil.copy2(label_fname, YOLO_DATA_PATH + \"labels/val/\")\n",
        "    else:\n",
        "        shutil.copy2(img_fname, YOLO_DATA_PATH + \"images/train/\")\n",
        "        shutil.copy2(label_fname, YOLO_DATA_PATH + \"labels/train/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdvfK8J23H9b"
      },
      "source": [
        "# Train the model with our custom dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1oFJey83WfR",
        "outputId": "5de2c948-b2cb-40a9-fa88-a16eef4a5c56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=../custom.yaml, hyp=data/hyps/hyp.scratch.yaml, epochs=15, batch_size=12, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, patience=100, freeze=0, save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0m⚠️ YOLOv5 is out of date by 172 commits. Use `git pull` or `git clone https://github.com/ultralytics/yolov5` to update.\n",
            "YOLOv5 🚀 v6.0-94-g47fac9f torch 1.10.0+cu102 CUDA:0 (NVIDIA GeForce GTX 1650 Ti with Max-Q Design, 3912MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 🚀 runs (RECOMMENDED)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Overriding model.yaml nc=80 with nc=15\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     53940  models.yolo.Detect                      [15, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model Summary: 270 layers, 7060084 parameters, 7060084 gradients, 16.0 GFLOPs\n",
            "\n",
            "Transferred 343/349 items from yolov5s.pt\n",
            "Scaled weight_decay = 0.00046875\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 57 weight, 60 weight (no decay), 60 bias\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'data/labels/train' images and labels...10 found, 0 missing, 0 e\u001b[0m\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: data/images/train/joshdw_20220217_19-12-29.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     93.023      124.12      121.46      155.22      139.27      91.163      165.58       121.2      158.94      91.429      171.96       109.5      186.05      86.113      198.27      106.31]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: data/images/train/joshdw_20220217_19-12-55.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     73.887      150.43      114.29      203.06      181.26      110.83      228.57      164.52      147.24      109.24      181.79      147.77      87.176      44.917      147.24]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: data/images/train/joshdw_20220217_19-13-35.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     111.63      146.98      148.84      190.83      213.16      104.98      243.19      152.82       141.4      111.63      171.16      148.04      82.126      92.492      148.04]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: data/images/train/joshdw_20220217_19-14-30.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     125.71      91.429      175.15      154.42      174.88      119.87      194.55      161.86      194.55      115.88      223.26      164.25]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: data/images/train/joshdw_20220217_19-15-08.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     157.08      99.402       229.9       180.2]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: data/images/train/joshdw_20220217_19-15-42.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     102.86      107.38      118.54      129.17      156.28       121.2      182.86      155.22      207.57       130.5      221.13      158.67      184.19      101.53      195.61      122.79]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: data/images/train/joshdw_20220217_19-16-22.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     212.89      130.76      245.58      178.07       168.5      121.99      209.44      160.27      124.65        90.1      144.58      116.41      229.63       85.05      247.71      115.35      93.289      64.851       109.5      88.239      66.179      56.611          80      70.432]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: data/images/train/joshdw_20220217_19-16-45.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     157.08      70.166      200.93         139      135.55      116.41      201.99      206.51      282.26      29.236      308.04      76.811]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: data/images/train/joshdw_20220217_19-17-23.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     173.29      94.884      205.71      126.25      215.81       111.1      243.19      144.32      188.17      118.27       210.5      147.51      169.57      123.06       190.3      146.98      152.29      116.68      172.23      141.13      118.54      117.21      142.99      148.31      104.45      113.49\n",
            "      128.64      136.88]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: data/images/train/joshdw_20220217_19-19-06.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      119.6       61.13      213.69      154.42      29.767      158.67      87.176      209.17      152.03      26.578      207.31      86.379]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: data/labels/train.cache\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/joshdw/Documents/src/FlockSentry/training/yolov5/train.py\", line 625, in <module>\n",
            "    main(opt)\n",
            "  File \"/home/joshdw/Documents/src/FlockSentry/training/yolov5/train.py\", line 522, in main\n",
            "    train(opt.hyp, opt, device, callbacks)\n",
            "  File \"/home/joshdw/Documents/src/FlockSentry/training/yolov5/train.py\", line 212, in train\n",
            "    train_loader, dataset = create_dataloader(train_path, imgsz, batch_size // WORLD_SIZE, gs, single_cls,\n",
            "  File \"/home/joshdw/Documents/src/FlockSentry/training/yolov5/utils/datasets.py\", line 101, in create_dataloader\n",
            "    dataset = LoadImagesAndLabels(path, imgsz, batch_size,\n",
            "  File \"/home/joshdw/Documents/src/FlockSentry/training/yolov5/utils/datasets.py\", line 436, in __init__\n",
            "    labels, shapes, self.segments = zip(*cache.values())\n",
            "ValueError: not enough values to unpack (expected 3, got 0)\n"
          ]
        }
      ],
      "source": [
        "!python3 train.py --img 640 --batch 12 --epochs 15 --data ../custom.yaml --weights yolov5s.pt --cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzYRiCdn3P1D"
      },
      "source": [
        "# Run model on something\n",
        "#### probably a video with chickens\n",
        "\n",
        "**NOTE:** You will need to modify `exp` to the latest exp folder generated from the training command above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YqNhqGUn3XCB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['runs/train/exp3/weights/best.pt'], source=/home/joshdw/Downloads/home/joshdw/Downloads/yt_download_dMW80rrTDb4.mp4, imgsz=[640, 640], conf_thres=0.3, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
            "YOLOv5 🚀 v6.0-94-g47fac9f torch 1.10.0+cu102 CUDA:0 (NVIDIA GeForce GTX 1650 Ti with Max-Q Design, 3912MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 213 layers, 7050580 parameters, 0 gradients, 15.9 GFLOPs\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/joshdw/Documents/src/FlockSentry/training/yolov5/detect.py\", line 244, in <module>\n",
            "    main(opt)\n",
            "  File \"/home/joshdw/Documents/src/FlockSentry/training/yolov5/detect.py\", line 239, in main\n",
            "    run(**vars(opt))\n",
            "  File \"/home/joshdw/.local/lib/python3.9/site-packages/torch/autograd/grad_mode.py\", line 28, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/joshdw/Documents/src/FlockSentry/training/yolov5/detect.py\", line 95, in run\n",
            "    dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt and not jit)\n",
            "  File \"/home/joshdw/Documents/src/FlockSentry/training/yolov5/utils/datasets.py\", line 170, in __init__\n",
            "    raise Exception(f'ERROR: {p} does not exist')\n",
            "Exception: ERROR: /home/joshdw/Downloads/home/joshdw/Downloads/yt_download_dMW80rrTDb4.mp4 does not exist\n"
          ]
        }
      ],
      "source": [
        "!python3 detect.py --weights runs/train/exp3/weights/best.pt --img 640 --conf 0.3 --source ~/Downloads/home/joshdw/Downloads/yt_download_dMW80rrTDb4.mp4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8kym4e_tbZy"
      },
      "source": [
        "# Copy final model to the drive\n",
        "\n",
        "The commands must be run in one line, otherwise the `NOW` variable will be destroyed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "J6sYN5DXte8-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘../gdrive/Shareddrives/FlockSentry/Models/17_Feb_2022-19_46/’: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!NOW=$(date +%d_%b_%Y-%H_%M) && mkdir ../gdrive/Shareddrives/FlockSentry/Models/\"$NOW\"/ && cp runs/train/exp/weights/last.pt ../gdrive/Shareddrives/FlockSentry/Models/\"$NOW\"/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "train_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5cc83896336e4342b157a6a1b6d43dfd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68592f9fe10440129343f09d49e4667b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e4fe94d643c4695be768c15ee1182e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7d4bcc8e79c94838a26f5b0c7ec87a0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "858681e2a3834c189245b1c38e9ae7c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1bf1d7f96d44ab0808650bd7d55c47d",
            "placeholder": "​",
            "style": "IPY_MODEL_7d4bcc8e79c94838a26f5b0c7ec87a0d",
            "value": " 780M/780M [00:16&lt;00:00, 35.7MB/s]"
          }
        },
        "cf756ea9ba02491b95c73853cc9325c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eebf44ba085c4a0f9f04e6b2c1d1aa9f",
            "placeholder": "​",
            "style": "IPY_MODEL_68592f9fe10440129343f09d49e4667b",
            "value": "100%"
          }
        },
        "e1bf1d7f96d44ab0808650bd7d55c47d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3ace0d513a24c2ca10ff09920646c7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff4aa7f3af734260a06b946a6205a6ba",
            "max": 818322941,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6e4fe94d643c4695be768c15ee1182e0",
            "value": 818322941
          }
        },
        "eebf44ba085c4a0f9f04e6b2c1d1aa9f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f64029bb3e57491690aa9ec75aa74fcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cf756ea9ba02491b95c73853cc9325c0",
              "IPY_MODEL_e3ace0d513a24c2ca10ff09920646c7d",
              "IPY_MODEL_858681e2a3834c189245b1c38e9ae7c5"
            ],
            "layout": "IPY_MODEL_5cc83896336e4342b157a6a1b6d43dfd"
          }
        },
        "ff4aa7f3af734260a06b946a6205a6ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
